{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee2a4b4b-4a27-4cbb-ba2e-2767d525ce07",
   "metadata": {},
   "source": [
    "## **AIM**\n",
    "\n",
    "**Linear Regression with Regularization (without using sklearn or equivalent library) and Simple and Multiple Linear Regression with and without regularization using Sklearn**\n",
    "\n",
    "**Apply it on datasets used in experiment 3.**\n",
    "\n",
    "**Compare outcome of experiment 3 and 4 and derive conclusions.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c943db-5c06-4bfc-b777-15f8e179b4a4",
   "metadata": {},
   "source": [
    "## **Without Sklearn and Without Regularization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2dd642-bcf5-4b07-b24e-dc3b0f56326b",
   "metadata": {},
   "source": [
    "**Simple Linear Regression with Batch Gradient Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "601100ab-9b82-4846-82d5-1272adee2818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch GD (LinearRegression) parameters:\n",
      "Theta 0: 7.2222\n",
      "Theta 1: 3.9937\n",
      "RMSE (Batch GD) on training data: 0.9962\n",
      "Predicted value for X=7 (Batch GD): 35.18\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate fake dataset\n",
    "np.random.seed(0)\n",
    "m = 100\n",
    "X = np.random.rand(m, 1) * 10\n",
    "y = 4*X + 7 + np.random.randn(m, 1)  # true relation with noise\n",
    "\n",
    "# ------------------------------\n",
    "# Batch Gradient Descent (sklearn LinearRegression)\n",
    "# ------------------------------\n",
    "model_batch = LinearRegression()\n",
    "model_batch.fit(X, y)\n",
    "\n",
    "theta0_batch = model_batch.intercept_[0]\n",
    "theta1_batch = model_batch.coef_[0][0]\n",
    "print(\"Batch GD (LinearRegression) parameters:\")\n",
    "print(f\"Theta 0: {theta0_batch:.4f}\")\n",
    "print(f\"Theta 1: {theta1_batch:.4f}\")\n",
    "\n",
    "# Predictions\n",
    "y_pred_batch = model_batch.predict(X)\n",
    "\n",
    "# RMSE\n",
    "rmse_batch = np.sqrt(mean_squared_error(y, y_pred_batch))\n",
    "print(f\"RMSE (Batch GD) on training data: {rmse_batch:.4f}\")\n",
    "\n",
    "# Predict for X=7\n",
    "pred_y_batch = model_batch.predict([[7]])\n",
    "print(f\"Predicted value for X=7 (Batch GD): {pred_y_batch[0,0]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90bb69d-99fb-4fc0-90e2-f4ab2d9e2238",
   "metadata": {},
   "source": [
    "**Simple Linear Regression with Stochastic Gradient Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afbe9274-fa9f-43ba-a724-6c8dd90b886b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Theta (coefficients):\n",
      " [[7.1095]\n",
      " [3.9871]]\n",
      "RMSE: 0.9048\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate Fake Dataset \n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "m = 100  # number of samples\n",
    "X = np.random.rand(m, 1) * 10  \n",
    "y = 4*X + 7 + np.random.randn(m, 1)  # y = 4*X + 7 + noise\n",
    "\n",
    "# Add bias column (x0 = 1)\n",
    "X_b = np.c_[np.ones((m, 1)), X]\n",
    "\n",
    "\n",
    "# Stochastic Gradient Descent\n",
    "\n",
    "eta = 0.01\n",
    "n_epochs = 50   \n",
    "\n",
    "theta = np.random.randn(2, 1)  \n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        xi = X_b[i:i+1]      \n",
    "        yi = y[i:i+1]       \n",
    "        gradient = 2 * xi.T.dot(xi.dot(theta) - yi)  \n",
    "        theta = theta - eta * gradient\n",
    "\n",
    "print(\"Final Theta (coefficients):\\n\", np.round(theta, 4))\n",
    "\n",
    "\n",
    "# RMSE\n",
    "y_pred = X_b.dot(theta)\n",
    "rmse = np.sqrt(np.mean((y - y_pred)**2))\n",
    "print(f\"RMSE: {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a71a42d-f93d-448d-87b4-dabf87cf84bc",
   "metadata": {},
   "source": [
    "**Simple Linear Regression with Mini Batch Gradient Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3d3cfcaf-7420-4e2f-a891-53ea025d791b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Theta (coefficients):\n",
      " [[7.0763]\n",
      " [3.9755]]\n",
      "RMSE: 0.9011\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Generate Fake Dataset\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "m = 100  # number of samples\n",
    "X = np.random.rand(m, 1) * 10  \n",
    "y = 4*X + 7 + np.random.randn(m, 1) \n",
    "\n",
    "# Add bias column (x0 = 1)\n",
    "X_b = np.c_[np.ones((m, 1)), X]\n",
    "\n",
    "\n",
    "# Mini-Batch Gradient Descent\n",
    "\n",
    "eta = 0.001# learning rate\n",
    "n_epochs = 1000    # number of passes over dataset\n",
    "batch_size = 15  # mini-batch size\n",
    "\n",
    "theta = np.random.randn(2, 1)  # initialize randomly\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Shuffle the data at the start of each epoch\n",
    "    indices = np.random.permutation(m)\n",
    "    X_b_shuffled = X_b[indices]\n",
    "    y_shuffled = y[indices]\n",
    "    \n",
    "    # Loop over mini-batches\n",
    "    for i in range(0, m, batch_size):\n",
    "        X_mini = X_b_shuffled[i:i+batch_size]\n",
    "        y_mini = y_shuffled[i:i+batch_size]\n",
    "        gradient = 2/len(y_mini) * X_mini.T.dot(X_mini.dot(theta) - y_mini)\n",
    "        theta = theta - eta * gradient\n",
    "\n",
    "print(\"Final Theta (coefficients):\\n\", np.round(theta, 4))\n",
    "\n",
    "\n",
    "# RMSE\n",
    "\n",
    "y_pred = X_b.dot(theta)\n",
    "rmse = np.sqrt(np.mean((y - y_pred)**2))\n",
    "print(f\"RMSE: {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3625efe0-5154-4786-8bd6-f6e8ee04a8e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f239680-68c2-4844-abfe-da2123f73597",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04dffd44-eb38-452d-af94-6b6fd75a9cf9",
   "metadata": {},
   "source": [
    "## **Without Sklearn and Without Regularization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d017e0-fa98-4e5b-bdcd-dcebaaa2eea2",
   "metadata": {},
   "source": [
    "**Multiple Linear Regression with Batch Gradient Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a314fd97-aa42-44a4-aba1-5db48e0105cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "θ = [52.5025, 6.9812, 16.8005, 0.3908, 0.7847, 0.5547]\n",
      "RMSE = 3.5309\n",
      "Predicted Performance Index: 63.0521\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# 1️⃣ Load dataset\n",
    "# -----------------------------\n",
    "data = pd.read_csv(\"Student_Performance.csv\") \n",
    "\n",
    "# Encode categorical feature\n",
    "data['Extracurricular Activities'] = data['Extracurricular Activities'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Features and target\n",
    "X = data[['Hours Studied', 'Previous Scores', 'Extracurricular Activities', \n",
    "          'Sleep Hours', 'Sample Question Papers Practiced']].values\n",
    "y = data[['Performance Index']].values\n",
    "\n",
    "# -----------------------------\n",
    "# 2️⃣ Feature scaling (manual standardization)\n",
    "# -----------------------------\n",
    "X_mean = np.mean(X, axis=0)\n",
    "X_std = np.std(X, axis=0)\n",
    "X_scaled = (X - X_mean) / X_std\n",
    "\n",
    "# Add bias column\n",
    "B = np.c_[np.ones((X_scaled.shape[0], 1)), X_scaled]\n",
    "\n",
    "# -----------------------------\n",
    "# 3️⃣ Initialize theta\n",
    "# -----------------------------\n",
    "np.random.seed(42)\n",
    "curr_theta = np.random.randn(B.shape[1], 1)\n",
    "\n",
    "# -----------------------------\n",
    "# 4️⃣ Hyperparameters\n",
    "# -----------------------------\n",
    "eta = 0.0003\n",
    "n_epochs = 5000\n",
    "m = X.shape[0]\n",
    "\n",
    "# -----------------------------\n",
    "# 5️⃣ Batch Gradient Descent\n",
    "# -----------------------------\n",
    "for i in range(n_epochs):\n",
    "    gradient = (2/m) * B.T.dot(B.dot(curr_theta) - y)\n",
    "    curr_theta = curr_theta - eta * gradient\n",
    "\n",
    "# -----------------------------\n",
    "# 6️⃣ Output results\n",
    "# -----------------------------\n",
    "theta_list = [float(round(x, 4)) for x in curr_theta.flatten()]\n",
    "print(\"θ =\", theta_list)\n",
    "\n",
    "# Predictions for training data\n",
    "y_pred = B.dot(curr_theta)\n",
    "\n",
    "# Compute RMSE\n",
    "rmse = np.sqrt(np.mean((y - y_pred) ** 2))\n",
    "print(f\"RMSE = {rmse:.4f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 7️⃣ Prediction for a new student\n",
    "# -----------------------------\n",
    "# Example: Hours Studied=5, Previous Scores=80, Extracurricular=Yes (1), Sleep=7, Sample Papers=3\n",
    "new_student_raw = np.array([[5, 80, 1, 7, 3]])\n",
    "new_student_scaled = (new_student_raw - X_mean) / X_std\n",
    "new_student_b = np.c_[np.ones((1, 1)), new_student_scaled]\n",
    "\n",
    "predicted_performance = new_student_b.dot(curr_theta)\n",
    "print(\"Predicted Performance Index:\", round(float(predicted_performance[0][0]), 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c89a39-3608-45d4-b6f8-2d70900eb2cc",
   "metadata": {},
   "source": [
    "**Multiple Linear Regression with Stochastic Gradient Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ee2bb20-074d-4568-a655-2a267e7d4c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "θ (SGD) = [55.4557, 7.2101, 17.6285, 0.2059, 0.8883, 0.3574]\n",
      "RMSE (SGD) = 2.0722\n",
      "Predicted Performance Index (SGD): 66.4605\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# 1️⃣ Load dataset\n",
    "# -----------------------------\n",
    "data = pd.read_csv(\"Student_Performance.csv\")\n",
    "\n",
    "# Encode categorical feature\n",
    "data['Extracurricular Activities'] = data['Extracurricular Activities'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Features and target\n",
    "X = data[['Hours Studied', 'Previous Scores', 'Extracurricular Activities', \n",
    "          'Sleep Hours', 'Sample Question Papers Practiced']].values\n",
    "y = data[['Performance Index']].values\n",
    "\n",
    "# -----------------------------\n",
    "# 2️⃣ Feature scaling (manual standardization)\n",
    "# -----------------------------\n",
    "X_mean = np.mean(X, axis=0)\n",
    "X_std = np.std(X, axis=0)\n",
    "X_scaled = (X - X_mean) / X_std\n",
    "\n",
    "# Add bias column\n",
    "B = np.c_[np.ones((X_scaled.shape[0], 1)), X_scaled]\n",
    "\n",
    "# -----------------------------\n",
    "# 3️⃣ Initialize theta\n",
    "# -----------------------------\n",
    "np.random.seed(42)\n",
    "curr_theta = np.random.randn(B.shape[1], 1)\n",
    "\n",
    "# -----------------------------\n",
    "# 4️⃣ Hyperparameters\n",
    "# -----------------------------\n",
    "eta = 0.005\n",
    "n_epochs = 100\n",
    "m = X.shape[0]\n",
    "\n",
    "# -----------------------------\n",
    "# 5️⃣ Stochastic Gradient Descent (SGD) without regularization\n",
    "# -----------------------------\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        xi = B[i].reshape(1, -1)  # single sample\n",
    "        yi = y[i].reshape(1, -1)\n",
    "        gradient = 2 * xi.T.dot(xi.dot(curr_theta) - yi)\n",
    "        curr_theta = curr_theta - eta * gradient\n",
    "\n",
    "# -----------------------------\n",
    "# 6️⃣ Output results\n",
    "# -----------------------------\n",
    "theta_list = [float(round(x, 4)) for x in curr_theta.flatten()]\n",
    "print(\"θ (SGD) =\", theta_list)\n",
    "\n",
    "# Predictions for training data\n",
    "y_pred = B.dot(curr_theta)\n",
    "\n",
    "# Compute RMSE\n",
    "rmse = np.sqrt(np.mean((y - y_pred) ** 2))\n",
    "print(f\"RMSE (SGD) = {rmse:.4f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 7️⃣ Prediction for a new student\n",
    "# -----------------------------\n",
    "# Example: Hours Studied=5, Previous Scores=80, Extracurricular=Yes (1), Sleep=7, Sample Papers=3\n",
    "new_student_raw = np.array([[5, 80, 1, 7, 3]])\n",
    "new_student_scaled = (new_student_raw - X_mean) / X_std\n",
    "new_student_b = np.c_[np.ones((1, 1)), new_student_scaled]\n",
    "\n",
    "predicted_performance = new_student_b.dot(curr_theta)\n",
    "print(\"Predicted Performance Index (SGD):\", round(float(predicted_performance[0][0]), 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafd6b10-0932-433d-9a13-6e13a9fca968",
   "metadata": {},
   "source": [
    "**Multiple Linear Regression with Mini-Batch Gradient Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d22aa4a2-e7c4-4f6d-962b-db5c35254004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "θ (Mini-Batch GD) = [55.1077, 7.2528, 17.6219, 0.2279, 0.6351, 0.6667]\n",
      "RMSE (Mini-Batch GD) = 2.0577\n",
      "Predicted Performance Index (Mini-Batch GD): 65.8901\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# 1️⃣ Load dataset\n",
    "# -----------------------------\n",
    "data = pd.read_csv(\"Student_Performance.csv\")\n",
    "\n",
    "# Encode categorical feature\n",
    "data['Extracurricular Activities'] = data['Extracurricular Activities'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Features and target\n",
    "X = data[['Hours Studied', 'Previous Scores', 'Extracurricular Activities', \n",
    "          'Sleep Hours', 'Sample Question Papers Practiced']].values\n",
    "y = data[['Performance Index']].values\n",
    "\n",
    "# -----------------------------\n",
    "# 2️⃣ Feature scaling (manual standardization)\n",
    "# -----------------------------\n",
    "X_mean = np.mean(X, axis=0)\n",
    "X_std = np.std(X, axis=0)\n",
    "X_scaled = (X - X_mean) / X_std\n",
    "\n",
    "# Add bias column\n",
    "B = np.c_[np.ones((X_scaled.shape[0], 1)), X_scaled]\n",
    "\n",
    "# -----------------------------\n",
    "# 3️⃣ Initialize theta\n",
    "# -----------------------------\n",
    "np.random.seed(42)\n",
    "curr_theta = np.random.randn(B.shape[1], 1)\n",
    "\n",
    "# -----------------------------\n",
    "# 4️⃣ Hyperparameters\n",
    "# -----------------------------\n",
    "eta = 0.005\n",
    "n_epochs = 50\n",
    "batch_size = 10\n",
    "m = X.shape[0]\n",
    "\n",
    "# -----------------------------\n",
    "# 5️⃣ Mini-Batch Gradient Descent (without regularization)\n",
    "# -----------------------------\n",
    "for epoch in range(n_epochs):\n",
    "    indices = np.random.permutation(m)\n",
    "    B_shuffled = B[indices]\n",
    "    y_shuffled = y[indices]\n",
    "    \n",
    "    for start in range(0, m, batch_size):\n",
    "        end = start + batch_size\n",
    "        xi = B_shuffled[start:end]  # mini-batch\n",
    "        yi = y_shuffled[start:end]\n",
    "        gradient = 2 * xi.T.dot(xi.dot(curr_theta) - yi)\n",
    "        curr_theta = curr_theta - eta * gradient\n",
    "\n",
    "# -----------------------------\n",
    "# 6️⃣ Output results\n",
    "# -----------------------------\n",
    "theta_list = [float(round(x, 4)) for x in curr_theta.flatten()]\n",
    "print(\"θ (Mini-Batch GD) =\", theta_list)\n",
    "\n",
    "# Predictions for training data\n",
    "y_pred = B.dot(curr_theta)\n",
    "\n",
    "# Compute RMSE\n",
    "rmse = np.sqrt(np.mean((y - y_pred) ** 2))\n",
    "print(f\"RMSE (Mini-Batch GD) = {rmse:.4f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 7️⃣ Prediction for a new student\n",
    "# -----------------------------\n",
    "new_student_raw = np.array([[5, 80, 1, 7, 3]])\n",
    "new_student_scaled = (new_student_raw - X_mean) / X_std\n",
    "new_student_b = np.c_[np.ones((1, 1)), new_student_scaled]\n",
    "\n",
    "predicted_performance = new_student_b.dot(curr_theta)\n",
    "print(\"Predicted Performance Index (Mini-Batch GD):\", round(float(predicted_performance[0][0]), 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76e6555-19aa-49a9-a1cc-38f5449e48a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d93fb18-11d4-4115-b68e-1ba459823dfa",
   "metadata": {},
   "source": [
    "## **Without Sklearn and With Regularization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee533c2f-4d87-4b44-9571-d6485e81233f",
   "metadata": {},
   "source": [
    "**Stochastic Gradient Descent with Lasso Regularization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf5711ed-673a-4c3c-a483-5461d6646f91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final parameters (theta):\n",
      "Theta 0 (bias): 7.2156\n",
      "Theta 1 (slope): 3.9531\n",
      "Predicted value for X=7: 34.89\n",
      "RMSE on training data: 0.8981\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Fake dataset\n",
    "m = 100  \n",
    "X = np.random.rand(m, 1) * 10\n",
    "y = 4 * X + 7 + np.random.randn(m, 1)  # true relation + noise\n",
    "\n",
    "# Add bias column (x0 = 1)\n",
    "X_b = np.c_[np.ones((m, 1)), X]\n",
    "\n",
    "# Hyperparameters\n",
    "eta = 0.01      # learning rate\n",
    "n_epochs = 1000  # epochs\n",
    "lambda_reg = 0.1 # Lasso regularization strength\n",
    "\n",
    "# Initialize theta randomly\n",
    "theta = np.random.randn(2, 1)\n",
    "\n",
    "# Stochastic Gradient Descent with Lasso\n",
    "for epoch in range(n_epochs):\n",
    "    indices = np.arange(m)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    for i in indices:\n",
    "        Xi = X_b[i:i+1]        # single sample\n",
    "        yi = y[i:i+1]\n",
    "        \n",
    "        # standard gradient\n",
    "        gradient = (2/m) * Xi.T.dot(Xi.dot(theta) - yi)\n",
    "        \n",
    "        # Lasso penalty (sign(theta)), exclude bias\n",
    "        lasso_term = (lambda_reg/m) * np.sign(theta)\n",
    "        lasso_term[0] = 0\n",
    "        \n",
    "        gradient += lasso_term\n",
    "        \n",
    "        # update step\n",
    "        theta -= eta * gradient\n",
    "\n",
    "print(\"Final parameters (theta):\")\n",
    "print(f\"Theta 0 (bias): {theta[0][0]:.4f}\")\n",
    "print(f\"Theta 1 (slope): {theta[1][0]:.4f}\")\n",
    "\n",
    "# Prediction for X=7\n",
    "pred = theta[0] + theta[1] * 7\n",
    "print(f\"Predicted value for X=7: {pred[0]:.2f}\")\n",
    "\n",
    "# RMSE on training data\n",
    "y_pred = X_b.dot(theta)\n",
    "rmse = np.sqrt(np.mean((y - y_pred) ** 2))\n",
    "print(f\"RMSE on training data: {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d7ca24-c8e5-43f5-b540-e29012eb11dd",
   "metadata": {},
   "source": [
    "**Batch Gradient Descent with Ridge Regularization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ee97f9a-b8fc-44b8-ad35-f492e2f764de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final parameters (theta):\n",
      "Theta 0 : 7.1906\n",
      "Theta 1 : 3.9577\n",
      "Predicted value for X=8: 38.85\n",
      "RMSE on training data: 0.8982\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "m = 100  \n",
    "X = np.random.rand(m, 1) * 10\n",
    "y = 4 * X + 7 + np.random.randn(m, 1)  \n",
    "\n",
    "# Add bias column (x0 = 1)\n",
    "X_b = np.c_[np.ones((m, 1)), X]\n",
    "\n",
    "\n",
    "eta = 0.01       # learning rate\n",
    "n_iter = 1000    # iterations\n",
    "lambda_re = 0.1 # Ridge regularization strength\n",
    "\n",
    "# Initialize theta randomly\n",
    "theta = np.random.randn(2, 1)\n",
    "\n",
    "# Batch Gradient Descent with Ridge\n",
    "for iteration in range(n_iter):\n",
    "    gradients = (2/m) * X_b.T.dot(X_b.dot(theta) - y)\n",
    "\n",
    "    # Ridge penalty (only for slope, not bias)\n",
    "    ridge_term = (lambda_reg/m) * theta\n",
    "    ridge_term[0] = 0   # do not regularize bias\n",
    "\n",
    "    gradients += ridge_term\n",
    "\n",
    "    theta -= eta * gradients\n",
    "\n",
    "print(\"Final parameters (theta):\")\n",
    "print(f\"Theta 0 : {theta[0][0]:.4f}\")\n",
    "print(f\"Theta 1 : {theta[1][0]:.4f}\")\n",
    "\n",
    "# Prediction for X=8\n",
    "pred = theta[0] + theta[1] * 8\n",
    "print(f\"Predicted value for X=8: {pred[0]:.2f}\")\n",
    "\n",
    "# RMSE on training data\n",
    "y_pred = X_b.dot(theta)\n",
    "rmse = np.sqrt(np.mean((y - y_pred) ** 2))\n",
    "print(f\"RMSE on training data: {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf4a7ad-b5ba-4a56-9da2-3b6321479b4b",
   "metadata": {},
   "source": [
    "**Mini-Batch Gradient Descent with Elastic Net Regularization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de30e174-5b4b-430d-b27f-36a048194d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final parameters (theta):\n",
      "Theta 0 (bias): 7.2187\n",
      "Theta 1 (slope): 3.9597\n",
      "Predicted value for X=7: 34.94\n",
      "RMSE on training data: 0.8988\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "m = 100  \n",
    "X = np.random.rand(m, 1) * 10\n",
    "y = 4 * X + 7 + np.random.randn(m, 1)\n",
    "\n",
    "# Add bias column (x0 = 1)\n",
    "X_b = np.c_[np.ones((m, 1)), X]\n",
    "\n",
    "# Hyperparameters\n",
    "eta = 0.05          # learning rate\n",
    "n_epochs = 1000     # epochs\n",
    "batch_size = 15     # mini-batch size\n",
    "alpha = 0.1         # overall regularization strength\n",
    "r = 0.5      # 0 = ridge, 1 = lasso\n",
    "\n",
    "# Initialize theta randomly\n",
    "theta = np.random.randn(2, 1)\n",
    "\n",
    "# Mini-Batch Gradient Descent with Elastic Net\n",
    "for epoch in range(n_epochs):\n",
    "    indices = np.arange(m)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    for start in range(0, m, batch_size):\n",
    "        end = start + batch_size\n",
    "        batch_idx = indices[start:end]\n",
    "        \n",
    "        Xi = X_b[batch_idx]\n",
    "        yi = y[batch_idx]\n",
    "        \n",
    "        # Standard gradient\n",
    "        gradient = (2/m) * Xi.T.dot(Xi.dot(theta) - yi)\n",
    "        \n",
    "        # Elastic Net penalty\n",
    "        l1_term = (alpha * r / m) * np.sign(theta)   # Lasso\n",
    "        l2_term = (alpha * (1 - r) / m) * theta      # Ridge\n",
    "        penalty = l1_term + l2_term\n",
    "        penalty[0] = 0  \n",
    "        \n",
    "        gradient += penalty\n",
    "        \n",
    "        theta -= eta * gradient\n",
    "\n",
    "print(\"Final parameters (theta):\")\n",
    "print(f\"Theta 0 (bias): {theta[0][0]:.4f}\")\n",
    "print(f\"Theta 1 (slope): {theta[1][0]:.4f}\")\n",
    "\n",
    "# Prediction for X=7\n",
    "pred = theta[0] + theta[1] * 7\n",
    "print(f\"Predicted value for X=7: {pred[0]:.2f}\")\n",
    "\n",
    "# RMSE on training data\n",
    "y_pred = X_b.dot(theta)\n",
    "rmse = np.sqrt(np.mean((y - y_pred) ** 2))\n",
    "print(f\"RMSE on training data: {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6997d5-81b8-449a-b6f9-55b6a46d7f2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3ec0615-2773-44af-aa4a-0dfca0a6997a",
   "metadata": {},
   "source": [
    "## **Without Sklearn and With Regularization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45428cc-4eb0-4f6f-94b2-1cfaef9d99f5",
   "metadata": {},
   "source": [
    "**Batch Gradient Descent with Ridge Regularization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5a6d6e2-5134-42ae-a5c4-c2ca8b83ecd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "θ = [54.857, 7.329, 17.544, 0.3201, 0.8129, 0.5591]\n",
      "RMSE = 2.0746\n",
      "Predicted Performance Index: 65.7938\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"Student_Performance.csv\")\n",
    "\n",
    "data['Extracurricular Activities'] = data['Extracurricular Activities'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Features and target\n",
    "X = data[['Hours Studied', 'Previous Scores', 'Extracurricular Activities', \n",
    "          'Sleep Hours', 'Sample Question Papers Practiced']].values\n",
    "y = data[['Performance Index']].values\n",
    "\n",
    "\n",
    "# Feature scaling (manual standardization)\n",
    "\n",
    "X_mean = np.mean(X, axis=0)\n",
    "X_std = np.std(X, axis=0)\n",
    "X_scaled = (X - X_mean) / X_std\n",
    "\n",
    "# Add bias column\n",
    "B = np.c_[np.ones((X_scaled.shape[0], 1)), X_scaled]\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "curr_theta = np.random.randn(B.shape[1], 1)\n",
    "\n",
    "eta = 0.0005       # learning rate\n",
    "n_epochs = 5000\n",
    "alpha = 0.01       # Ridge regularization strength\n",
    "m = X.shape[0]\n",
    "\n",
    "\n",
    "# Ridge Batch Gradient Descent\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    # Compute gradient\n",
    "    gradient = (2/m) * B.T.dot(B.dot(curr_theta) - y)\n",
    "    gradient[1:] += (2 * alpha / m) * curr_theta[1:]  # regularize only weights, not bias\n",
    "    # Update theta\n",
    "    curr_theta = curr_theta - eta * gradient\n",
    "\n",
    "\n",
    "# Final theta\n",
    "theta_list = [float(round(x, 4)) for x in curr_theta.flatten()]\n",
    "print(\"θ =\", theta_list)\n",
    "\n",
    "# Predictions for training data\n",
    "y_pred = B.dot(curr_theta)\n",
    "\n",
    "# Compute RMSE\n",
    "rmse = np.sqrt(np.mean((y - y_pred) ** 2))\n",
    "print(f\"RMSE = {rmse:.4f}\")\n",
    "\n",
    "\n",
    "#  Prediction for a new student\n",
    "\n",
    "# Example: Hours Studied=5, Previous Scores=80, Extracurricular=Yes (1), Sleep=7, Sample Papers=3\n",
    "new_student_raw = np.array([[5, 80, 1, 7, 3]])\n",
    "\n",
    "new_student_scaled = (new_student_raw - X_mean) / X_std\n",
    "new_student_b = np.c_[np.ones((1, 1)), new_student_scaled]\n",
    "\n",
    "predicted_performance = new_student_b.dot(curr_theta)\n",
    "print(\"Predicted Performance Index:\", round(float(predicted_performance[0][0]), 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e337ae5-766a-4b77-9cc0-cac4609d6136",
   "metadata": {},
   "source": [
    "**Stochastic Gradient Descent with Lasso Regularization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c374eac8-5a52-4b8d-b9f4-ba28f3d00a01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "θ = [55.2368, 7.3879, 17.6651, 0.2918, 0.8224, 0.5564]\n",
      "RMSE = 2.0376\n",
      "Predicted Performance Index: 66.2231\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"Student_Performance.csv\")\n",
    "\n",
    "data['Extracurricular Activities'] = data['Extracurricular Activities'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Features and target\n",
    "X = data[['Hours Studied', 'Previous Scores', 'Extracurricular Activities', \n",
    "          'Sleep Hours', 'Sample Question Papers Practiced']].values\n",
    "y = data[['Performance Index']].values\n",
    "\n",
    "\n",
    "# Feature scaling (manual standardization)\n",
    "\n",
    "X_mean = np.mean(X, axis=0)\n",
    "X_std = np.std(X, axis=0)\n",
    "X_scaled = (X - X_mean) / X_std\n",
    "\n",
    "# Add bias column\n",
    "B = np.c_[np.ones((X_scaled.shape[0], 1)), X_scaled]\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "curr_theta = np.random.randn(B.shape[1], 1)\n",
    "\n",
    "eta = 0.0001       # learning rate\n",
    "n_epochs = 50\n",
    "alpha = 0.1        # Lasso regularization strength\n",
    "m = X.shape[0]\n",
    "\n",
    "\n",
    "# Stochastic Gradient Descent with Lasso (L1)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        xi = B[i].reshape(1, -1)  # single sample\n",
    "        yi = y[i].reshape(1, -1)\n",
    "        gradient = 2 * xi.T.dot(xi.dot(curr_theta) - yi)\n",
    "        # L1 regularization (exclude bias)\n",
    "        gradient[1:] += (alpha/m) * np.sign(curr_theta[1:])\n",
    "        curr_theta = curr_theta - eta * gradient\n",
    "\n",
    "theta_list = [float(round(x, 4)) for x in curr_theta.flatten()]\n",
    "print(\"θ =\", theta_list)\n",
    "\n",
    "# Predictions for training data\n",
    "y_pred = B.dot(curr_theta)\n",
    "\n",
    "# Compute RMSE\n",
    "rmse = np.sqrt(np.mean((y - y_pred) ** 2))\n",
    "print(f\"RMSE = {rmse:.4f}\")\n",
    "\n",
    "# Prediction for a new student\n",
    "# Example: Hours Studied=5, Previous Scores=80, Extracurricular=Yes (1), Sleep=7, Sample Papers=3\n",
    "new_student_raw = np.array([[5, 80, 1, 7, 3]])\n",
    "new_student_scaled = (new_student_raw - X_mean) / X_std\n",
    "new_student_b = np.c_[np.ones((1, 1)), new_student_scaled]\n",
    "\n",
    "predicted_performance = new_student_b.dot(curr_theta)\n",
    "print(\"Predicted Performance Index:\", round(float(predicted_performance[0][0]), 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f272fc24-ee87-4531-aa4e-d64c9c6803ac",
   "metadata": {},
   "source": [
    "**Mini-Batch Gradient Descent with Elastic Net Regularization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67613562-5f88-48b4-8635-f595cf2caf5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "θ = [55.222, 7.3867, 17.6605, 0.3069, 0.8146, 0.5567]\n",
      "RMSE = 2.0375\n",
      "Predicted Performance Index: 66.2183\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"Student_Performance.csv\")\n",
    "\n",
    "\n",
    "data['Extracurricular Activities'] = data['Extracurricular Activities'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Features and target\n",
    "X = data[['Hours Studied', 'Previous Scores', 'Extracurricular Activities', \n",
    "          'Sleep Hours', 'Sample Question Papers Practiced']].values\n",
    "y = data[['Performance Index']].values\n",
    "\n",
    "\n",
    "# Feature scaling (manual standardization)\n",
    "\n",
    "X_mean = np.mean(X, axis=0)\n",
    "X_std = np.std(X, axis=0)\n",
    "X_scaled = (X - X_mean) / X_std\n",
    "\n",
    "# Add bias column\n",
    "B = np.c_[np.ones((X_scaled.shape[0], 1)), X_scaled]\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "curr_theta = np.random.randn(B.shape[1], 1)\n",
    "\n",
    "\n",
    "eta = 0.00001\n",
    "n_epochs = 50\n",
    "batch_size = 10\n",
    "alpha1 = 0.05  # L1 strength (Lasso)\n",
    "alpha2 = 0.05  # L2 strength (Ridge)\n",
    "m = X.shape[0]\n",
    "\n",
    "\n",
    "# Mini-Batch Gradient Descent with Elastic Net\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    indices = np.random.permutation(m)\n",
    "    B_shuffled = B[indices]\n",
    "    y_shuffled = y[indices]\n",
    "    \n",
    "    for start in range(0, m, batch_size):\n",
    "        end = start + batch_size\n",
    "        xi = B_shuffled[start:end]\n",
    "        yi = y_shuffled[start:end]\n",
    "        \n",
    "        gradient = 2 * xi.T.dot(xi.dot(curr_theta) - yi)\n",
    "        # Elastic Net regularization (exclude bias)\n",
    "        gradient[1:] += (2 * alpha2 / m) * curr_theta[1:] + (alpha1 / m) * np.sign(curr_theta[1:])\n",
    "        \n",
    "        curr_theta = curr_theta - eta * gradient\n",
    "\n",
    "theta_list = [float(round(x, 4)) for x in curr_theta.flatten()]\n",
    "print(\"θ =\", theta_list)\n",
    "\n",
    "# Predictions for training data\n",
    "y_pred = B.dot(curr_theta)\n",
    "\n",
    "# Compute RMSE\n",
    "rmse = np.sqrt(np.mean((y - y_pred) ** 2))\n",
    "print(f\"RMSE = {rmse:.4f}\")\n",
    "\n",
    "\n",
    "# Prediction for a new student\n",
    "\n",
    "new_student_raw = np.array([[5, 80, 1, 7, 3]])\n",
    "new_student_scaled = (new_student_raw - X_mean) / X_std\n",
    "new_student_b = np.c_[np.ones((1, 1)), new_student_scaled]\n",
    "\n",
    "predicted_performance = new_student_b.dot(curr_theta)\n",
    "print(\"Predicted Performance Index:\", round(float(predicted_performance[0][0]), 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddab191-541b-4c72-a79a-ca01fa127193",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "105e7cb9-0f54-41ae-b9da-1960b16da0f4",
   "metadata": {},
   "source": [
    "## **Wit Sklearn and No Regularization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166fcb96-8a07-4322-9b3c-e051f48df1a0",
   "metadata": {},
   "source": [
    "**Simple Linear Regression with Batch Gradient Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ab7a21c-f8e3-42d4-a94d-2a27e5f084b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final parameters (theta) using Batch Gradient Descent:\n",
      "Theta 0: 7.1896\n",
      "Theta 1: 3.9988\n",
      "RMSE on training data: 0.9964\n",
      "Predicted value for X=7: 35.18\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate fake dataset\n",
    "np.random.seed(0)\n",
    "m = 100\n",
    "X = np.random.rand(m, 1) * 10\n",
    "y = 4*X + 7 + np.random.randn(m, 1)  # true relation with noise\n",
    "\n",
    "# Add bias column (x0 = 1)\n",
    "X_b = np.c_[np.ones((m, 1)), X]\n",
    "\n",
    "# Initialize parameters\n",
    "theta = np.random.randn(2, 1)  # random initialization for [theta0, theta1]\n",
    "\n",
    "# Hyperparameters\n",
    "eta = 0.01\n",
    "n_iter = 1000\n",
    "\n",
    "# Batch Gradient Descent\n",
    "for iteration in range(n_iter):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = theta - eta * gradients\n",
    "\n",
    "# Final parameters\n",
    "theta0, theta1 = theta[0, 0], theta[1, 0]\n",
    "print(\"Final parameters (theta) using Batch Gradient Descent:\")\n",
    "print(f\"Theta 0: {theta0:.4f}\")\n",
    "print(f\"Theta 1: {theta1:.4f}\")\n",
    "\n",
    "# Predictions for training data\n",
    "y_pred = X_b.dot(theta)\n",
    "\n",
    "# RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "print(f\"RMSE on training data: {rmse:.4f}\")\n",
    "\n",
    "# Predict salary for 7 years experience (or X=7)\n",
    "x_new = np.array([[1, 7]])  # include bias term\n",
    "pred_y = x_new.dot(theta)\n",
    "print(f\"Predicted value for X=7: {pred_y[0,0]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5765795-736d-4725-abba-9762c18df68c",
   "metadata": {},
   "source": [
    "**Simple Linear Regression with Stochastic Gradient Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3946601-9a5a-493f-9af8-59adb04b76c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters using SGDRegressor:\n",
      "Theta 0: 7.1999\n",
      "Theta 1: 4.0404\n",
      "RMSE on training data: 1.0246\n",
      "Predicted value for X=7: 35.48\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate fake dataset\n",
    "np.random.seed(0)\n",
    "m = 100\n",
    "X = np.random.rand(m, 1) * 10\n",
    "y = 4*X + 7 + np.random.randn(m, 1)  # true relation with noise\n",
    "y = y.ravel()  # SGDRegressor expects 1D target\n",
    "\n",
    "# Create SGD Regressor\n",
    "model = SGDRegressor(\n",
    "    max_iter=1000,       # number of epochs\n",
    "    learning_rate='constant',\n",
    "    eta0=0.01,           # learning rate\n",
    "    penalty=None,        # no regularization\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train model\n",
    "model.fit(X, y)\n",
    "\n",
    "# Get parameters\n",
    "theta0 = model.intercept_[0]\n",
    "theta1 = model.coef_[0]\n",
    "print(\"Parameters using SGDRegressor:\")\n",
    "print(f\"Theta 0: {theta0:.4f}\")\n",
    "print(f\"Theta 1: {theta1:.4f}\")\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "print(f\"RMSE on training data: {rmse:.4f}\")\n",
    "\n",
    "# Predict for X=7\n",
    "pred_y = model.predict([[7]])\n",
    "print(f\"Predicted value for X=7: {pred_y[0]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77326095-d6f9-487a-a32a-623b76b03ce6",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c1132a-1b92-47bc-afa8-f2c132214a33",
   "metadata": {},
   "source": [
    "**Simple Linear Regression with Mini-Batch Gradient Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ddf265d-2079-4ae2-8883-37523a28acd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final parameters (Mini-Batch GD on random data):\n",
      "Theta 0: 7.2153\n",
      "Theta 1: 4.0030\n",
      "Predicted value for X=7: 35.24\n",
      "RMSE on training data: 0.9384\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate fake dataset\n",
    "m = 100\n",
    "X = np.random.rand(m, 1) * 10      # Feature (YearsExperience)\n",
    "y = 4*X + 7 + np.random.randn(m, 1)  # True relation + noise\n",
    "y = y.ravel()  # SGDRegressor expects 1D target\n",
    "\n",
    "# Initialize SGDRegressor for Mini-Batch GD\n",
    "model = SGDRegressor(\n",
    "    max_iter=1,\n",
    "    learning_rate='constant',\n",
    "    eta0=0.01,\n",
    "    penalty=None,        # no regularization\n",
    "    random_state=42,\n",
    "    warm_start=True\n",
    ")\n",
    "\n",
    "n_epochs = 1000\n",
    "batch_size = 30\n",
    "\n",
    "# Mini-Batch training loop\n",
    "for epoch in range(n_epochs):\n",
    "    indices = np.arange(m)\n",
    "    np.random.shuffle(indices)  # shuffle data each epoch\n",
    "    \n",
    "    for start in range(0, m, batch_size):\n",
    "        end = start + batch_size\n",
    "        batch_idx = indices[start:end]\n",
    "        X_batch = X[batch_idx]\n",
    "        y_batch = y[batch_idx]\n",
    "        \n",
    "        model.partial_fit(X_batch, y_batch)\n",
    "\n",
    "# Get parameters\n",
    "theta0 = model.intercept_[0]\n",
    "theta1 = model.coef_[0]\n",
    "print(\"Final parameters (Mini-Batch GD on random data):\")\n",
    "print(f\"Theta 0: {theta0:.4f}\")\n",
    "print(f\"Theta 1: {theta1:.4f}\")\n",
    "\n",
    "# Predict value for X=7\n",
    "pred_y = model.predict([[7]])\n",
    "print(f\"Predicted value for X=7: {pred_y[0]:.2f}\")\n",
    "\n",
    "# Compute RMSE on training data\n",
    "y_pred = model.predict(X)\n",
    "rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "print(f\"RMSE on training data: {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e493a7-bc16-4e20-b0f5-2af126b770a6",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ff95bc-3420-4b5c-9879-33f434d6f76a",
   "metadata": {},
   "source": [
    "## **Wit Sklearn and No Regularization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0c6e03-46b3-4523-abae-1e512f8a5174",
   "metadata": {},
   "source": [
    "**Multiple Linear Regression with Batch Gradient Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddb60bcc-62f9-4d56-8c87-a9c4b4d30027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients (theta): [ 7.3869 17.662   0.3064  0.8149  0.5557]\n",
      "Intercept (theta0): 55.2248\n",
      "RMSE on training data: 2.0375\n",
      "Predicted Performance Index: 66.2223\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv(\"Student_Performance.csv\")\n",
    "\n",
    "# Encode categorical variable\n",
    "data['Extracurricular Activities'] = data['Extracurricular Activities'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Features and target\n",
    "X = data[['Hours Studied', 'Previous Scores', 'Extracurricular Activities', \n",
    "          'Sleep Hours', 'Sample Question Papers Practiced']].values\n",
    "y = data[['Performance Index']].values\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Linear Regression on standardized features\n",
    "model = LinearRegression()\n",
    "model.fit(X_scaled, y)\n",
    "\n",
    "# Print coefficients (theta)\n",
    "print(\"Coefficients (theta):\", np.round(model.coef_[0], 4))\n",
    "print(\"Intercept (theta0):\", np.round(model.intercept_[0], 4))\n",
    "\n",
    "# Predictions on training data\n",
    "y_pred = model.predict(X_scaled)\n",
    "rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "print(\"RMSE on training data:\", np.round(rmse, 4))\n",
    "\n",
    "# Prediction for a new student\n",
    "new_student = np.array([[5, 80, 1, 7, 3]])\n",
    "new_student_scaled = scaler.transform(new_student)\n",
    "predicted_performance = model.predict(new_student_scaled)\n",
    "print(\"Predicted Performance Index:\", np.round(predicted_performance[0][0], 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c967cb38-58f2-4a3d-a8b8-d18785f25d48",
   "metadata": {},
   "source": [
    "**Multiple Linear Regression with Stochastic Gradient Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "754c62b9-d24c-4c2f-8d03-0fd786cf97f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients (theta): [ 7.6044 17.6104  0.2578  0.8618  0.3464]\n",
      "Intercept (theta0): 54.9472\n",
      "RMSE on training data: 2.0799\n",
      "Predicted Performance Index: 65.9934\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv(\"Student_Performance.csv\")\n",
    "\n",
    "# Encode categorical variable\n",
    "data['Extracurricular Activities'] = data['Extracurricular Activities'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Features and target\n",
    "X = data[['Hours Studied', 'Previous Scores', 'Extracurricular Activities', \n",
    "          'Sleep Hours', 'Sample Question Papers Practiced']].values\n",
    "y = data['Performance Index'].values  # 1D array for SGDRegressor\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# SGD Regressor (Stochastic Gradient Descent) WITHOUT regularization\n",
    "sgd_model = SGDRegressor(\n",
    "    max_iter=1000,        # number of epochs\n",
    "    learning_rate='constant',\n",
    "    eta0=0.01,            # adjust as needed for convergence\n",
    "    penalty=None,         # no regularization\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Fit model\n",
    "sgd_model.fit(X_scaled, y)\n",
    "\n",
    "# Coefficients and intercept\n",
    "print(\"Coefficients (theta):\", np.round(sgd_model.coef_, 4))\n",
    "print(\"Intercept (theta0):\", np.round(sgd_model.intercept_[0], 4))\n",
    "\n",
    "# RMSE on training data\n",
    "y_pred = sgd_model.predict(X_scaled)\n",
    "rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "print(\"RMSE on training data:\", np.round(rmse, 4))\n",
    "\n",
    "# Prediction for a new student\n",
    "new_student = np.array([[5, 80, 1, 7, 3]])\n",
    "new_student_scaled = scaler.transform(new_student)\n",
    "predicted_performance = sgd_model.predict(new_student_scaled)\n",
    "print(\"Predicted Performance Index:\", np.round(predicted_performance[0], 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a56562-5eae-49e3-a766-f24be964572b",
   "metadata": {},
   "source": [
    "**Multiple Linear Regression with Mini-Batch Gradient Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d10b564-4bad-475e-98c6-91865607ae04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [ 7.32943194 17.54003076  0.30992607  0.81353109  0.56162621]\n",
      "Intercept: [54.85273187]\n",
      "Predicted Performance Index: 65.77569603867418\n",
      "RMSE on Training Data: 2.0755248390066705\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv(\"Student_Performance.csv\")\n",
    "\n",
    "# Encode categorical feature\n",
    "data['Extracurricular Activities'] = data['Extracurricular Activities'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Features and target\n",
    "X = data[['Hours Studied', 'Previous Scores', 'Extracurricular Activities', \n",
    "          'Sleep Hours', 'Sample Question Papers Practiced']].values\n",
    "y = data['Performance Index'].values\n",
    "\n",
    "# Optional: scale features for better convergence\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# SGD Regressor for Mini-Batch Gradient Descent\n",
    "sgd = SGDRegressor(max_iter=1,          # one epoch at a time\n",
    "                   eta0=0.00001,        # learning rate\n",
    "                   learning_rate='constant',\n",
    "                   penalty=None,        # no regularization\n",
    "                   warm_start=True,     # continue training over multiple calls to fit\n",
    "                   shuffle=True)\n",
    "\n",
    "batch_size = 10\n",
    "n_epochs = 50\n",
    "m = X.shape[0]\n",
    "\n",
    "# Mini-Batch Training\n",
    "for epoch in range(n_epochs):\n",
    "    indices = np.random.permutation(m)\n",
    "    X_shuffled = X_scaled[indices]\n",
    "    y_shuffled = y[indices]\n",
    "    \n",
    "    for start in range(0, m, batch_size):\n",
    "        end = start + batch_size\n",
    "        xi = X_shuffled[start:end]\n",
    "        yi = y_shuffled[start:end]\n",
    "        sgd.partial_fit(xi, yi)\n",
    "\n",
    "print(\"Coefficients:\", sgd.coef_)\n",
    "print(\"Intercept:\", sgd.intercept_)\n",
    "\n",
    "# Predict for new student\n",
    "new_student = np.array([[5, 80, 1, 7, 3]])\n",
    "new_student_scaled = scaler.transform(new_student)\n",
    "predicted_performance = sgd.predict(new_student_scaled)\n",
    "print(\"Predicted Performance Index:\", predicted_performance[0])\n",
    "\n",
    "# RMSE on training data\n",
    "y_pred = sgd.predict(X_scaled)\n",
    "rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "print(\"RMSE on Training Data:\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1b32bd-8f86-4cc2-8e60-6e7e4d3c322d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d371455-e3ba-437b-9177-288eb3764cf8",
   "metadata": {},
   "source": [
    "## **With Sklearn and With Regularization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b05e9a5-bf42-4cb2-b633-5d57c808a09b",
   "metadata": {},
   "source": [
    "**Batch Gradient Descent with Ridge Regularization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76ec29f8-284e-4932-9a8e-6b2c042aacb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final parameters (theta) using sklearn Ridge:\n",
      "Theta 0 (bias): 7.2224\n",
      "Theta 1 (slope): 3.9936\n",
      "RMSE on training data: 0.9962\n",
      "Predicted value for X=7: 35.18\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate fake dataset\n",
    "np.random.seed(0)\n",
    "m = 100\n",
    "X = np.random.rand(m, 1) * 10\n",
    "y = 4*X + 7 + np.random.randn(m, 1)  # true relation with noise\n",
    "\n",
    "# Train Ridge Regression model\n",
    "ridge_reg = Ridge(alpha=0.01, fit_intercept=True)  # alpha = λ\n",
    "ridge_reg.fit(X, y)\n",
    "\n",
    "# Extract parameters as scalars\n",
    "theta0 = ridge_reg.intercept_.item()  # bias\n",
    "theta1 = ridge_reg.coef_[0]           # slope\n",
    "print(\"Final parameters (theta) using sklearn Ridge:\")\n",
    "print(f\"Theta 0 (bias): {theta0:.4f}\")\n",
    "print(f\"Theta 1 (slope): {theta1:.4f}\")\n",
    "\n",
    "# Predictions for training data\n",
    "y_pred = ridge_reg.predict(X)\n",
    "\n",
    "# RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "print(f\"RMSE on training data: {rmse:.4f}\")\n",
    "\n",
    "# Predict salary for 7 years experience (or X=7)\n",
    "pred_y = ridge_reg.predict([[7]])\n",
    "print(f\"Predicted value for X=7: {pred_y[0]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a3cbf6-aa02-4c35-9544-c73ddab1b831",
   "metadata": {},
   "source": [
    "**Stochastic Gradient Descent with Lasso Regularization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3808394b-a7df-4f34-b850-7859b2a91ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic SGD Lasso (sklearn) final parameters:\n",
      "Theta 0 (bias): 7.3159\n",
      "Theta 1 (slope): 3.9809\n",
      "RMSE: 0.9975\n",
      "Predicted value for X=7: 35.18\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# ========================\n",
    "# 1. Generate fake dataset\n",
    "# ========================\n",
    "np.random.seed(0)\n",
    "m = 100\n",
    "X = np.random.rand(m, 1) * 10\n",
    "y = 4*X + 7 + np.random.randn(m, 1)\n",
    "y = y.ravel()  # flatten\n",
    "\n",
    "# ========================\n",
    "# 2. Standardize features\n",
    "# ========================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ========================\n",
    "# 3. SGD Regressor for Lasso\n",
    "# ========================\n",
    "# penalty='l1' → Lasso, learning_rate='constant' for fixed learning rate\n",
    "sgd_lasso = SGDRegressor(\n",
    "    penalty='l1',       # Lasso\n",
    "    alpha=0.01,         # regularization strength\n",
    "    max_iter=1000,      # total epochs\n",
    "    learning_rate='constant',\n",
    "    eta0=0.01,          # learning rate\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "sgd_lasso.fit(X_scaled, y)\n",
    "\n",
    "# ========================\n",
    "# 4. Convert parameters to original scale\n",
    "# ========================\n",
    "theta1 = sgd_lasso.coef_[0] / scaler.scale_[0]\n",
    "theta0 = sgd_lasso.intercept_[0] - theta1 * scaler.mean_[0]\n",
    "\n",
    "print(\"Stochastic SGD Lasso (sklearn) final parameters:\")\n",
    "print(f\"Theta 0 (bias): {theta0:.4f}\")\n",
    "print(f\"Theta 1 (slope): {theta1:.4f}\")\n",
    "\n",
    "# ========================\n",
    "# 5. RMSE on training data\n",
    "# ========================\n",
    "y_pred = sgd_lasso.predict(X_scaled)\n",
    "rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "\n",
    "# ========================\n",
    "# 6. Predict for X=7\n",
    "# ========================\n",
    "x_test_scaled = scaler.transform([[7]])\n",
    "pred_y = sgd_lasso.predict(x_test_scaled)\n",
    "print(f\"Predicted value for X=7: {pred_y[0]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afee8ed-d15a-4bfb-ba27-7e0649ebbc47",
   "metadata": {},
   "source": [
    "**Mini-Batch Gradient Descent with Elastic Net Regularization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c17995dd-4a8a-4ce9-94e3-d1f03cfb7be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mini-batch SGD Elastic Net final parameters:\n",
      "Theta 0 (bias): 7.3477\n",
      "Theta 1 (slope): 3.9681\n",
      "RMSE on testing data: 0.9550\n",
      "Predicted value for X=7: 35.12\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# ========================\n",
    "# 1. Generate fake dataset\n",
    "# ========================\n",
    "np.random.seed(0)\n",
    "m = 100\n",
    "X = np.random.rand(m, 1) * 10\n",
    "y = 4*X + 7 + np.random.randn(m, 1)\n",
    "y = y.ravel()  # flatten\n",
    "\n",
    "# ========================\n",
    "# 2. Train-test split (optional)\n",
    "# ========================\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ========================\n",
    "# 3. Standardize features\n",
    "# ========================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ========================\n",
    "# 4. SGD Regressor for Elastic Net\n",
    "# ========================\n",
    "sgd_elastic = SGDRegressor(\n",
    "    penalty='elasticnet',  # Elastic Net\n",
    "    l1_ratio=0.5,          # ratio of L1 to L2 \n",
    "    alpha=0.01,            # regularization strength\n",
    "    max_iter=1000,         # epochs\n",
    "    learning_rate='constant',\n",
    "    eta0=0.01,             # learning rate\n",
    "    random_state=42,\n",
    "    shuffle=True           # enables mini-batch behavior\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "sgd_elastic.fit(X_train_scaled, y_train)\n",
    "\n",
    "# ========================\n",
    "# 5. Convert parameters to original scale\n",
    "# ========================\n",
    "theta1 = sgd_elastic.coef_[0] / scaler.scale_[0]\n",
    "theta0 = sgd_elastic.intercept_[0] - theta1 * scaler.mean_[0]\n",
    "\n",
    "print(\"Mini-batch SGD Elastic Net final parameters:\")\n",
    "print(f\"Theta 0 (bias): {theta0:.4f}\")\n",
    "print(f\"Theta 1 (slope): {theta1:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# ========================\n",
    "# RMSE on testing data\n",
    "# ========================\n",
    "y_test_pred = sgd_elastic.predict(X_test_scaled)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "print(f\"RMSE on testing data: {rmse_test:.4f}\")\n",
    "\n",
    "# ========================\n",
    "# 8. Predict for X=7\n",
    "# ========================\n",
    "x_test_scaled = scaler.transform([[7]])\n",
    "pred_y = sgd_elastic.predict(x_test_scaled)\n",
    "print(f\"Predicted value for X=7: {pred_y[0]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3728d195-e3c9-4c4a-9a63-82a8108457df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42279241-62f1-4583-8461-57f6c49d4c19",
   "metadata": {},
   "source": [
    "## **Without Sklearn and With Regularization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91af647-fea9-46d1-a9d6-ced0729c5d26",
   "metadata": {},
   "source": [
    "**Batch Gradient Descent with Ridge Regularization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "786ff952-1d49-4dd4-adb1-569d5a2db9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta : [55.3115      7.401246   17.63704981  0.30428766  0.81002151  0.54883857]\n",
      " MSE: 4.0827\n",
      " RMSE: 2.0206\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# =======================\n",
    "# 1. Load and Prepare Data\n",
    "# =======================\n",
    "data = pd.read_csv(\"Student_Performance.csv\")\n",
    "\n",
    "# Convert Yes/No to 1/0\n",
    "data[\"Extracurricular Activities\"] = data[\"Extracurricular Activities\"].map({\"Yes\": 1, \"No\": 0})\n",
    "\n",
    "X = data[[\n",
    "    \"Hours Studied\",\n",
    "    \"Previous Scores\",\n",
    "    \"Extracurricular Activities\",\n",
    "    \"Sleep Hours\",\n",
    "    \"Sample Question Papers Practiced\"\n",
    "]].values\n",
    "y = data[\"Performance Index\"].values\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# =======================\n",
    "# 2. Standardize Features\n",
    "# =======================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "# =======================\n",
    "# 3. Ridge Regression\n",
    "# =======================\n",
    "alpha = 0.1  # regularization parameter\n",
    "ridge_model = Ridge(alpha=alpha, solver='auto', max_iter=1500, random_state=42)\n",
    "ridge_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# =======================\n",
    "# 4. Get Coefficients (theta)\n",
    "# =======================\n",
    "theta = np.r_[ridge_model.intercept_, ridge_model.coef_]  # include bias as theta_0\n",
    "print(\"Theta :\", theta)\n",
    "\n",
    "# =======================\n",
    "# 5. Evaluate Model\n",
    "# =======================\n",
    "y_pred = ridge_model.predict(X_test_scaled)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\" MSE: {mse:.4f}\")\n",
    "print(f\" RMSE: {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf4fbc6-7c1f-4880-9759-1f5ee8e531df",
   "metadata": {},
   "source": [
    "**Stochastic Gradient Descent with Lasso Regularization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18a86171-3d7b-4d80-82fd-2b2240afd8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta (intercept + coefficients): [55.30595436  7.48410452 17.54367898  0.13035214  0.49154551  0.74818848]\n",
      "Test MSE: 4.2969, Test RMSE: 2.0729\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# =======================\n",
    "# 1. Load and Prepare Data\n",
    "# =======================\n",
    "data = pd.read_csv(\"Student_Performance.csv\")\n",
    "\n",
    "# Convert Yes/No to 1/0\n",
    "data[\"Extracurricular Activities\"] = data[\"Extracurricular Activities\"].map({\"Yes\": 1, \"No\": 0})\n",
    "\n",
    "X = data[[\n",
    "    \"Hours Studied\",\n",
    "    \"Previous Scores\",\n",
    "    \"Extracurricular Activities\",\n",
    "    \"Sleep Hours\",\n",
    "    \"Sample Question Papers Practiced\"\n",
    "]].values\n",
    "y = data[\"Performance Index\"].values\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# =======================\n",
    "# 2. Standardize Features\n",
    "# =======================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "# =======================\n",
    "# 3. Lasso Regression using SGD\n",
    "# =======================\n",
    "alpha = 0.1  # regularization strength (like lambda in manual Lasso)\n",
    "n_epochs = 1500\n",
    "learning_rate = 0.01\n",
    "\n",
    "lasso_sgd = SGDRegressor(\n",
    "    penalty='l1',        # Lasso\n",
    "    alpha=alpha,\n",
    "    max_iter=n_epochs,\n",
    "    learning_rate='constant',\n",
    "    eta0=learning_rate,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "lasso_sgd.fit(X_train_scaled, y_train)\n",
    "\n",
    "# =======================\n",
    "# 4. Get Coefficients (theta)\n",
    "# =======================\n",
    "theta = np.r_[lasso_sgd.intercept_, lasso_sgd.coef_]  # intercept + coefficients\n",
    "print(\"Theta (intercept + coefficients):\", theta)\n",
    "\n",
    "# =======================\n",
    "# 5. Evaluate Model\n",
    "# =======================\n",
    "\n",
    "y_test_pred = lasso_sgd.predict(X_test_scaled)\n",
    "\n",
    "\n",
    "\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "\n",
    "print(f\"Test MSE: {test_mse:.4f}, Test RMSE: {test_rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0364ff93-b018-4554-af51-9e54d010687e",
   "metadata": {},
   "source": [
    "**Mini-Batch Gradient Descent with Elastic Net Regularization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a481fb9b-a178-4d6d-b11d-883a24232fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta (intercept + coefficients): [55.21669077  6.83117165 16.89344438  0.30812886  0.58569076  0.48996788]\n",
      "Test MSE: 5.1998, Test RMSE: 2.2803\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# =======================\n",
    "# 1. Load and Prepare Data\n",
    "# =======================\n",
    "data = pd.read_csv(\"Student_Performance.csv\")\n",
    "data[\"Extracurricular Activities\"] = data[\"Extracurricular Activities\"].map({\"Yes\": 1, \"No\": 0})\n",
    "\n",
    "X = data[[\n",
    "    \"Hours Studied\",\n",
    "    \"Previous Scores\",\n",
    "    \"Extracurricular Activities\",\n",
    "    \"Sleep Hours\",\n",
    "    \"Sample Question Papers Practiced\"\n",
    "]].values\n",
    "y = data[\"Performance Index\"].values\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# =======================\n",
    "# 2. Standardize Features\n",
    "# =======================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "# =======================\n",
    "# 3. Elastic Net Regression with Mini-Batch SGD\n",
    "# =======================\n",
    "alpha = 0.1       # regularization strength\n",
    "l1_ratio = 0.5    # L1 vs L2 ratio\n",
    "learning_rate = 0.01\n",
    "n_epochs = 300\n",
    "batch_size = 64   # mini-batch size\n",
    "\n",
    "elasticnet_sgd = SGDRegressor(\n",
    "    penalty='elasticnet',\n",
    "    alpha=0.1,\n",
    "    l1_ratio=0.5,\n",
    "    max_iter=500,\n",
    "    learning_rate='constant',\n",
    "    eta0=0.01,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "elasticnet_sgd.fit(X_train_scaled, y_train)\n",
    "\n",
    "\n",
    "# Mini-batch training\n",
    "for epoch in range(n_epochs):\n",
    "    permutation = np.random.permutation(len(X_train_scaled))\n",
    "    X_shuffled = X_train_scaled[permutation]\n",
    "    y_shuffled = y_train[permutation]\n",
    "    \n",
    "    for i in range(0, len(X_train_scaled), batch_size):\n",
    "        X_batch = X_shuffled[i:i+batch_size]\n",
    "        y_batch = y_shuffled[i:i+batch_size]\n",
    "        elasticnet_sgd.partial_fit(X_batch, y_batch)\n",
    "\n",
    "\n",
    "theta = np.r_[elasticnet_sgd.intercept_, elasticnet_sgd.coef_]  # intercept + coefficients\n",
    "print(\"Theta (intercept + coefficients):\", theta)\n",
    "\n",
    "\n",
    "y_test_pred = elasticnet_sgd.predict(X_test_scaled)\n",
    "\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "\n",
    "print(f\"Test MSE: {test_mse:.4f}, Test RMSE: {test_rmse:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ce57c3-0ecd-4487-8e47-e59ea91873d2",
   "metadata": {},
   "source": [
    "**Comparision**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfd6fa4-a448-49cb-b07c-b30239e4ae80",
   "metadata": {},
   "source": [
    "### **Without Using Scikit-Learn — Without Regularization**\n",
    "\n",
    "| Method                     | Simple Linear Regression (θ, RMSE)                     | Multiple Linear Regression (θ, RMSE)                                 |\n",
    "|---------------------------|---------------------------------------------------------|-----------------------------------------------------------------------|\n",
    "| **Batch Gradient Descent** | θ = [7.1896, 3.9988]<br>RMSE = 0.9964                  | θ = [52.5025, 6.9812, 16.8005, 0.3908, 0.7847, 0.5547]<br>RMSE = 3.5309 |\n",
    "| **Stochastic Gradient Descent** | θ = [7.1095, 3.9871]<br>RMSE = 0.9048            | θ = [55.4557, 7.2101, 17.6285, 0.2059, 0.8883, 0.3574]<br>RMSE = 2.0722 |\n",
    "| **Mini-Batch Gradient Descent** | θ = [7.0763, 3.9755]<br>RMSE = 0.9011            | θ = [55.1077, 7.2528, 17.6219, 0.2279, 0.6351, 0.6667]<br>RMSE = 2.0577 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d200f21-da49-4a02-b0e6-ea93fd8aa356",
   "metadata": {},
   "source": [
    "### **Without Using Scikit-Learn — With Regularization**\n",
    "\n",
    "| Method                     | Simple Linear Regression (θ, RMSE)                      | Multiple Linear Regression (θ, RMSE)                                 |\n",
    "|---------------------------|----------------------------------------------------------|-----------------------------------------------------------------------|\n",
    "| **Batch Gradient Descent** | θ = [7.1906, 3.9577]<br>RMSE = 0.8982                   | θ = [54.857, 7.329, 17.544, 0.3201, 0.8129, 0.5591]<br>RMSE = 2.0746 |\n",
    "| **Stochastic Gradient Descent** | θ = [7.2156, 3.9531]<br>RMSE = 0.8981             | θ = [55.2368, 7.3879, 17.6651, 0.2918, 0.8224, 0.5564]<br>RMSE = 2.0376 |\n",
    "| **Mini-Batch Gradient Descent** | θ = [7.2187, 3.9597]<br>RMSE = 0.8988             | θ = [55.222, 7.3867, 17.6605, 0.3069, 0.8146, 0.5567]<br>RMSE = 2.0375 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ad2de8-20bf-4ba3-976c-a0e225ede022",
   "metadata": {},
   "source": [
    "### **Using Scikit-Learn — Without Regularization**\n",
    "\n",
    "| Method                     | Simple Linear Regression (θ, RMSE)                   | Multiple Linear Regression (θ, RMSE)                                  |\n",
    "|---------------------------|-------------------------------------------------------|------------------------------------------------------------------------|\n",
    "| **Batch Gradient Descent** | θ = [7.2222, 3.9937]<br>RMSE = 0.9962               | θ = [55.2248, 7.3869, 17.662, 0.3064, 0.8149, 0.5557]<br>RMSE = 2.0375 |\n",
    "| **Stochastic Gradient Descent** | θ = [7.1999, 4.0404]<br>RMSE = 1.0246         | θ = [54.9472, 7.6044, 17.6104, 0.2578, 0.8618, 0.3464]<br>RMSE = 2.0799 |\n",
    "| **Mini-Batch Gradient Descent** | θ = [7.2153, 4.0030]<br>RMSE = 0.9384         | θ = [55.1077, 7.2528, 17.6219, 0.2279, 0.6351, 0.6667]<br>RMSE = 2.0755 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fee42f-df5b-4081-85ed-0869772a5c8a",
   "metadata": {},
   "source": [
    "### **Using Scikit-Learn — With Regularization**\n",
    "\n",
    "| Method                     | Simple Linear Regression (θ, RMSE)                     | Multiple Linear Regression (θ, RMSE)                                   |\n",
    "|---------------------------|----------------------------------------------------------|-------------------------------------------------------------------------|\n",
    "| **Batch Gradient Descent** | θ = [7.2224, 3.9936]<br>RMSE = 0.9962                  | θ = [55.3115, 7.4012, 17.6370, 0.3042, 0.8100, 0.5488]<br>RMSE = 2.0206 |\n",
    "| **Stochastic Gradient Descent** | θ = [7.3159, 3.9809]<br>RMSE = 0.9975            | θ = [55.3059, 7.4841, 17.5436, 0.1303, 0.4915, 0.7481]<br>RMSE = 2.0729 |\n",
    "| **Mini-Batch Gradient Descent** | θ = [7.2917, 3.9689]<br>RMSE = 0.9561            | θ = [55.1671, 7.0243, 16.6474, 0.1142, 0.5722, 0.6656]<br>RMSE = 2.026 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbe1eb6-ef3e-4041-b50a-3ae4af31329a",
   "metadata": {},
   "source": [
    "## **Conclusion**\n",
    "\n",
    "The experiments clearly show that **regularization significantly improves linear regression models** by reducing overfitting and improving generalization.\n",
    "\n",
    "Without regularization, models consistently produced **higher RMSE** (e.g., 3.5309), indicating poor learning and overfitting.\n",
    "\n",
    "With regularization applied:\n",
    "\n",
    "- RMSE dropped notably  \n",
    "- Model complexity was controlled  \n",
    "- Overfitting reduced  \n",
    "- Predictions became more reliable  \n",
    "\n",
    "The best performance occurred in **Multiple Linear Regression using scikit-learn with regularization**, achieving the lowest RMSE of **2.0206**.\n",
    "\n",
    "Overall, **regularization is the most critical strategy** for building accurate and robust linear regression models.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
